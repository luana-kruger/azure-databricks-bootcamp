# Desafio de Projeto 2: C√≥pia de Dados de SQL On-Premises para Blob Storage com Azure Data Factory

Este projeto faz parte do bootcamp **Microsoft AI for Tech - Azure Databricks**, promovido pela **DIO** em parceria com a **Microsoft**.

## üéØ Objetivo

O objetivo desta atividade pr√°tica √© simular um processo de **redund√¢ncia e movimenta√ß√£o de dados** utilizando o **Azure Data Factory**. O fluxo consiste em copiar dados de uma tabela SQL Server on-premises para um **container Blob Storage**, estruturando os arquivos em formato `.txt` dentro da **camada prata** de um Data Lake.

<p align="center">
<img src="images/schema_adf_azure.png" alt="infobox" width="50%" height="50%">
</p>



## üõ†Ô∏è Etapas do Projeto

### 1. Configura√ß√µes iniciais no Data Factory

- Acesse o menu lateral **"Gerenciar"** dentro do Azure Data Factory.
- Configure os **Servi√ßos Vinculados (Linked Services)**:
  
  #### a) SQL Server On-Premises
  - Criar um recurso de **Integration Runtime** para acessar dados locais.
  - Criar um **Linked Service** apontando para o SQL Server local (utiliza o IR acima).
  
  #### b) Azure SQL Database
  - Criar um banco de dados no Azure (opcional).
  - Criar um **Linked Service** para esse banco (sem necessidade de IR).

  #### c) Blob Storage
  - Criar um recurso do tipo **Storage Account**.
  - Criar **containers** para as camadas do Data Lake:
    - `bronze/`
    - `silver/`
    - `gold/`
  - Criar um **Linked Service** apontando para o Blob Storage.


<p align="center">
<img src="images/linked_services.png" alt="infobox" width="60%" height="60%">
</p>

---

### 2. Cria√ß√£o do Pipeline de C√≥pia

- Acesse o menu lateral **"Autor"** no Data Factory e clique em **"Pipelines"** > **Novo pipeline**.
- Em **Atividades**, selecione a atividade **"Copy data"** (Mover e Transformar).

#### Configura√ß√£o da atividade:

- **Origem:**
  - Criar um **Dataset** do tipo SQL Server.
  - Associar ao Linked Service criado anteriormente.
  - Selecionar a tabela que ser√° copiada (on-premises).

<p align="center">
<img src="images/pipeline_source.png" alt="infobox" width="60%" height="60%">
</p>

- **Destino:**
  - Criar um **Dataset** do tipo **DelimitedText**.
  - Associar ao Linked Service do Blob Storage.
  - Definir o caminho de destino para a **camada prata**.
  - Escolher formato `.txt` com delimitadores apropriados.

<p align="center">
<img src="images/pipeline_sink.png" alt="infobox" width="60%" height="60%">
</p>

---

### 3. Valida√ß√£o, Publica√ß√£o e Execu√ß√£o

- Ap√≥s configurar os datasets e os par√¢metros de c√≥pia:
  - Clique em **Validar** para verificar inconsist√™ncias.
  - Em seguida, **Publicar todas** as altera√ß√µes.
  - Por fim, **Execute** o pipeline manualmente.



## üìÅ Estrutura Esperada no Blob Storage

<p align="center">
<img src="images/blob_storage.png" alt="infobox" width="60%" height="60%">
</p>




## üìå Considera√ß√µes Finais

Foi poss√≠vel observar um cen√°rio completo de integra√ß√£o entre ambientes **on-premises** e a **nuvem Azure**, incluindo:

- A estrutura m√≠nima para ingest√£o de dados com o Azure Data Factory.
- O uso do **Integration Runtime** para conex√µes locais.
- A separa√ß√£o em **camadas de dados** para futuras arquiteturas de Data Lake.



## üîó Refer√™ncias

- [Azure Data Factory - Vis√£o Geral](https://learn.microsoft.com/pt-br/azure/data-factory/introduction)
- [Integration Runtime (IR) no Data Factory](https://learn.microsoft.com/pt-br/azure/data-factory/concepts-integration-runtime)
- [Linked Services no ADF](https://learn.microsoft.com/pt-br/azure/data-factory/concepts-linked-services?tabs=data-factory)
- [Datasets no ADF](https://learn.microsoft.com/pt-br/azure/data-factory/concepts-datasets-linked-services)

---